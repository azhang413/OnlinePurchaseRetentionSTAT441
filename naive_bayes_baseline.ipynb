{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7e789e",
   "metadata": {},
   "source": [
    "# Naive Bayes BaselineThis notebook combines the pure-Python Naive Bayes implementation with the commentary from `NAIVE_BAYES_NOTES.md`.Run the cell below to train/test the model using the offline UCI dataset (`data/online_shoppers_intention.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e77891c2",
   "metadata": {
    "name": "naive-bayes-code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully imported all required libraries\n",
      "  - csv, math, random, pathlib, typing, matplotlib\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úì Successfully imported all required libraries\")\n",
    "print(\"  - csv, math, random, pathlib, typing, matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlyz78bjam",
   "metadata": {},
   "source": [
    "**Import necessary libraries for file handling, mathematical operations, randomization, and type hints.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "jy8fn0bd2z",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Defined feature types:\n",
      "  - Categorical columns (7): Browser, Month, OperatingSystems, Region, TrafficType, VisitorType, Weekend\n",
      "  - Target column: Revenue\n"
     ]
    }
   ],
   "source": [
    "CATEGORY_COLUMNS = {\n",
    "    \"Month\",\n",
    "    \"OperatingSystems\",\n",
    "    \"Browser\",\n",
    "    \"Region\",\n",
    "    \"TrafficType\",\n",
    "    \"VisitorType\",\n",
    "    \"Weekend\",\n",
    "}\n",
    "\n",
    "TARGET_COL = \"Revenue\"\n",
    "\n",
    "print(\"‚úì Defined feature types:\")\n",
    "print(f\"  - Categorical columns ({len(CATEGORY_COLUMNS)}): {', '.join(sorted(CATEGORY_COLUMNS))}\")\n",
    "print(f\"  - Target column: {TARGET_COL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1gzw5umdb3s",
   "metadata": {},
   "source": [
    "**Define which columns are categorical vs numerical, and specify the target column (Revenue).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3jjctw0fc3g",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path: Path) -> Tuple[List[List[float]], List[int], List[str]]:\n",
    "    \"\"\"Load dataset from CSV and separate features from labels.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LOADING DATASET FROM: {csv_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    with csv_path.open(newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        columns = [c for c in reader.fieldnames if c != TARGET_COL]\n",
    "        records, labels = [], []\n",
    "        \n",
    "        for row in reader:\n",
    "            features: List = []\n",
    "            for col in columns:\n",
    "                val = row[col]\n",
    "                if col in CATEGORY_COLUMNS:\n",
    "                    features.append(val.strip())\n",
    "                else:\n",
    "                    features.append(float(val))\n",
    "            \n",
    "            label = 1 if row[TARGET_COL].strip().lower() == \"true\" else 0\n",
    "            records.append(features)\n",
    "            labels.append(label)\n",
    "    \n",
    "    num_positive = sum(labels)\n",
    "    num_negative = len(labels) - num_positive\n",
    "    \n",
    "    print(f\"‚úì Dataset loaded successfully!\")\n",
    "    print(f\"  - Total samples: {len(records)}\")\n",
    "    print(f\"  - Total features: {len(columns)}\")\n",
    "    print(f\"  - Numerical features: {len([c for c in columns if c not in CATEGORY_COLUMNS])}\")\n",
    "    print(f\"  - Categorical features: {len([c for c in columns if c in CATEGORY_COLUMNS])}\")\n",
    "    print(f\"  - Class distribution:\")\n",
    "    print(f\"    ‚Ä¢ Positive (Revenue=True): {num_positive} ({num_positive/len(labels)*100:.2f}%)\")\n",
    "    print(f\"    ‚Ä¢ Negative (Revenue=False): {num_negative} ({num_negative/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    return records, labels, columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nehvxoeyel8",
   "metadata": {},
   "source": [
    "**Load the dataset from CSV file. Categorical features are kept as strings, numerical features are converted to floats. The target column (Revenue) is converted to binary (1 for True, 0 for False).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aig89il1hc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(\n",
    "    data: List[List], labels: List[int], test_ratio: float = 0.2\n",
    ") -> Tuple[List[List], List[int], List[List], List[int]]:\n",
    "    \"\"\"Split data into train/test sets while maintaining class distribution.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PERFORMING STRATIFIED TRAIN/TEST SPLIT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    grouped = {0: [], 1: []}\n",
    "    for feat, lab in zip(data, labels):\n",
    "        grouped[lab].append((feat, lab))\n",
    "    \n",
    "    print(f\"Class distribution before split:\")\n",
    "    print(f\"  - Class 0 (Negative): {len(grouped[0])} samples\")\n",
    "    print(f\"  - Class 1 (Positive): {len(grouped[1])} samples\")\n",
    "    \n",
    "    for group in grouped.values():\n",
    "        random.shuffle(group)\n",
    "    \n",
    "    train_X, train_y, test_X, test_y = [], [], [], []\n",
    "    for cls, items in grouped.items():\n",
    "        split = int(len(items) * (1 - test_ratio))\n",
    "        for feat, lab in items[:split]:\n",
    "            train_X.append(feat)\n",
    "            train_y.append(lab)\n",
    "        for feat, lab in items[split:]:\n",
    "            test_X.append(feat)\n",
    "            test_y.append(lab)\n",
    "    \n",
    "    print(f\"\\n‚úì Split complete!\")\n",
    "    print(f\"  - Train set: {len(train_X)} samples ({(1-test_ratio)*100:.0f}%)\")\n",
    "    print(f\"    ‚Ä¢ Class 0: {train_y.count(0)} samples\")\n",
    "    print(f\"    ‚Ä¢ Class 1: {train_y.count(1)} samples\")\n",
    "    print(f\"  - Test set: {len(test_X)} samples ({test_ratio*100:.0f}%)\")\n",
    "    print(f\"    ‚Ä¢ Class 0: {test_y.count(0)} samples\")\n",
    "    print(f\"    ‚Ä¢ Class 1: {test_y.count(1)} samples\")\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yveiwp88zv",
   "metadata": {},
   "source": [
    "**Perform stratified train/test split to maintain the same class distribution in both sets. Default is 80/20 split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "vsfnqo2t2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \"\"\"Naive Bayes classifier for mixed numerical and categorical features.\"\"\"\n",
    "    \n",
    "    def __init__(self, numeric_idx: List[int], categorical_idx: List[int]):\n",
    "        \"\"\"Initialize with indices of numeric and categorical columns.\"\"\"\n",
    "        self.numeric_idx = numeric_idx\n",
    "        self.categorical_idx = categorical_idx\n",
    "        self.priors: Dict[int, float] = {}\n",
    "        self.num_stats: Dict[int, Dict[int, Tuple[float, float]]] = {}\n",
    "        self.cat_counts: Dict[int, Dict[int, Dict[str, int]]] = {}\n",
    "        self.cat_totals: Dict[int, Dict[int, int]] = {}\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"INITIALIZING NAIVE BAYES CLASSIFIER\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  - Numerical features: {len(numeric_idx)}\")\n",
    "        print(f\"  - Categorical features: {len(categorical_idx)}\")\n",
    "    \n",
    "    def fit(self, X: List[List], y: List[int]):\n",
    "        \"\"\"Train the Naive Bayes model on the training data.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TRAINING NAIVE BAYES MODEL\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        n = len(y)\n",
    "        class_indices: Dict[int, List[int]] = {0: [], 1: []}\n",
    "        for idx, label in enumerate(y):\n",
    "            class_indices[label].append(idx)\n",
    "        \n",
    "        for cls, indices in class_indices.items():\n",
    "            # Calculate prior probability for each class\n",
    "            self.priors[cls] = len(indices) / n\n",
    "            print(f\"\\nClass {cls}:\")\n",
    "            print(f\"  - Prior probability: {self.priors[cls]:.4f}\")\n",
    "            print(f\"  - Number of samples: {len(indices)}\")\n",
    "            \n",
    "            # Calculate mean and variance for numerical features\n",
    "            self.num_stats[cls] = {}\n",
    "            for col in self.numeric_idx:\n",
    "                values = [X[i][col] for i in indices]\n",
    "                mean = sum(values) / len(values)\n",
    "                var = sum((v - mean) ** 2 for v in values) / max(len(values) - 1, 1)\n",
    "                self.num_stats[cls][col] = (mean, var if var > 1e-6 else 1e-6)\n",
    "            \n",
    "            print(f\"  - Calculated statistics for {len(self.numeric_idx)} numerical features\")\n",
    "            \n",
    "            # Count categorical feature values\n",
    "            self.cat_counts.setdefault(cls, {})\n",
    "            self.cat_totals.setdefault(cls, {})\n",
    "            total_categories = 0\n",
    "            for col in self.categorical_idx:\n",
    "                counts: Dict[str, int] = {}\n",
    "                for i in indices:\n",
    "                    key = X[i][col]\n",
    "                    counts[key] = counts.get(key, 0) + 1\n",
    "                self.cat_counts[cls][col] = counts\n",
    "                self.cat_totals[cls][col] = sum(counts.values())\n",
    "                total_categories += len(counts)\n",
    "            \n",
    "            print(f\"  - Counted {total_categories} unique categorical values across {len(self.categorical_idx)} features\")\n",
    "        \n",
    "        print(f\"\\n‚úì Model training complete!\")\n",
    "    \n",
    "    def _gaussian_log_prob(self, value: float, mean: float, var: float) -> float:\n",
    "        \"\"\"Calculate log probability for a Gaussian distribution.\"\"\"\n",
    "        return -0.5 * (\n",
    "            math.log(2 * math.pi * var) + ((value - mean) ** 2) / var\n",
    "        )\n",
    "    \n",
    "    def predict_row(self, row: List) -> int:\n",
    "        \"\"\"Predict the class label for a single row.\"\"\"\n",
    "        log_posteriors = {}\n",
    "        for cls in self.priors:\n",
    "            log_prob = math.log(self.priors[cls])\n",
    "            \n",
    "            # Add log probabilities for numerical features\n",
    "            for col in self.numeric_idx:\n",
    "                mean, var = self.num_stats[cls][col]\n",
    "                log_prob += self._gaussian_log_prob(row[col], mean, var)\n",
    "            \n",
    "            # Add log probabilities for categorical features\n",
    "            for col in self.categorical_idx:\n",
    "                counts = self.cat_counts[cls][col]\n",
    "                total = self.cat_totals[cls][col]\n",
    "                value = row[col]\n",
    "                count = counts.get(value, 0)\n",
    "                smoothing = 1.0\n",
    "                vocab_size = len(counts)\n",
    "                log_prob += math.log((count + smoothing) / (total + smoothing * vocab_size))\n",
    "            \n",
    "            log_posteriors[cls] = log_prob\n",
    "        \n",
    "        return max(log_posteriors, key=log_posteriors.get)\n",
    "    \n",
    "    def predict(self, X: List[List]) -> List[int]:\n",
    "        \"\"\"Predict class labels for multiple rows.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MAKING PREDICTIONS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  - Number of samples to predict: {len(X)}\")\n",
    "        \n",
    "        predictions = [self.predict_row(row) for row in X]\n",
    "        \n",
    "        num_positive = sum(predictions)\n",
    "        num_negative = len(predictions) - num_positive\n",
    "        \n",
    "        print(f\"  - Predicted class distribution:\")\n",
    "        print(f\"    ‚Ä¢ Class 0 (Negative): {num_negative} ({num_negative/len(predictions)*100:.2f}%)\")\n",
    "        print(f\"    ‚Ä¢ Class 1 (Positive): {num_positive} ({num_positive/len(predictions)*100:.2f}%)\")\n",
    "        print(f\"‚úì Predictions complete!\")\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gb7evszj1s4",
   "metadata": {},
   "source": [
    "**Complete Naive Bayes classifier implementation including:**\n",
    "- **Initialization**: Set up feature indices and storage for statistics\n",
    "- **Training (fit)**: Calculate prior probabilities, mean/variance for numerical features, frequency counts for categorical features\n",
    "- **Prediction**: Use Gaussian log probability for numerical features and Laplace smoothing for categorical features to predict class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52er98jet54",
   "metadata": {},
   "source": [
    "# Model Execution & Analysis\n",
    "\n",
    "This section executes the Naive Bayes model step-by-step, allowing you to observe and analyze each stage of the machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0wcofrga3l",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "l26d6xaw8z",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "               NAIVE BAYES BASELINE\n",
      "============================================================\n",
      "‚úì Random seed set to 441 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(441)\n",
    "print(\"=\"*60)\n",
    "print(\" \"*15 + \"NAIVE BAYES BASELINE\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úì Random seed set to 441 for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30mppmpj843",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "n8nlt596ja",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING DATASET FROM: /Users/zhuqiang/Downloads/online_shoppers_intention 2.csv\n",
      "============================================================\n",
      "‚úì Dataset loaded successfully!\n",
      "  - Total samples: 12330\n",
      "  - Total features: 17\n",
      "  - Numerical features: 10\n",
      "  - Categorical features: 7\n",
      "  - Class distribution:\n",
      "    ‚Ä¢ Positive (Revenue=True): 1908 (15.47%)\n",
      "    ‚Ä¢ Negative (Revenue=False): 10422 (84.53%)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from Downloads folder\n",
    "data_path = Path.home() / \"Downloads\" / \"online_shoppers_intention 2.csv\"\n",
    "X, y, columns = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4mi19fmzt",
   "metadata": {},
   "source": [
    "## 3. Feature Type Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57xb5x1zt74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE TYPE SEPARATION\n",
      "============================================================\n",
      "  - Numerical feature indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  - Categorical feature indices: [10, 11, 12, 13, 14, 15, 16]\n",
      "\n",
      "  - Numerical feature names: ['Administrative', 'Administrative_Duration', 'Informational', 'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration', 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay']\n",
      "  - Categorical feature names: ['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend']\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and categorical column indices\n",
    "numeric_idx = [\n",
    "    idx\n",
    "    for idx, name in enumerate(columns)\n",
    "    if name not in CATEGORY_COLUMNS\n",
    "]\n",
    "categorical_idx = [\n",
    "    idx\n",
    "    for idx, name in enumerate(columns)\n",
    "    if name in CATEGORY_COLUMNS\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FEATURE TYPE SEPARATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Numerical feature indices: {numeric_idx}\")\n",
    "print(f\"  - Categorical feature indices: {categorical_idx}\")\n",
    "print(f\"\\n  - Numerical feature names: {[columns[i] for i in numeric_idx]}\")\n",
    "print(f\"  - Categorical feature names: {[columns[i] for i in categorical_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k5ecj9dvt8",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "kdkb447aka8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMING STRATIFIED TRAIN/TEST SPLIT\n",
      "============================================================\n",
      "Class distribution before split:\n",
      "  - Class 0 (Negative): 10422 samples\n",
      "  - Class 1 (Positive): 1908 samples\n",
      "\n",
      "‚úì Split complete!\n",
      "  - Train set: 9863 samples (80%)\n",
      "    ‚Ä¢ Class 0: 8337 samples\n",
      "    ‚Ä¢ Class 1: 1526 samples\n",
      "  - Test set: 2467 samples (20%)\n",
      "    ‚Ä¢ Class 0: 2085 samples\n",
      "    ‚Ä¢ Class 1: 382 samples\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets (80/20 split)\n",
    "X_train, y_train, X_test, y_test = stratified_split(X, y, test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0kdazkj4p47",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81r14q590jo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING NAIVE BAYES CLASSIFIER\n",
      "============================================================\n",
      "  - Numerical features: 10\n",
      "  - Categorical features: 7\n",
      "\n",
      "============================================================\n",
      "TRAINING NAIVE BAYES MODEL\n",
      "============================================================\n",
      "\n",
      "Class 0:\n",
      "  - Prior probability: 0.8453\n",
      "  - Number of samples: 8337\n",
      "  - Calculated statistics for 10 numerical features\n",
      "  - Counted 64 unique categorical values across 7 features\n",
      "\n",
      "Class 1:\n",
      "  - Prior probability: 0.1547\n",
      "  - Number of samples: 1526\n",
      "  - Calculated statistics for 10 numerical features\n",
      "  - Counted 59 unique categorical values across 7 features\n",
      "\n",
      "‚úì Model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the Naive Bayes model\n",
    "nb = NaiveBayes(numeric_idx=numeric_idx, categorical_idx=categorical_idx)\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uayzj2wzeap",
   "metadata": {},
   "source": [
    "## 6. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "zaofsqvdqb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MAKING PREDICTIONS\n",
      "============================================================\n",
      "  - Number of samples to predict: 2467\n",
      "  - Predicted class distribution:\n",
      "    ‚Ä¢ Class 0 (Negative): 1890 (76.61%)\n",
      "    ‚Ä¢ Class 1 (Positive): 577 (23.39%)\n",
      "‚úì Predictions complete!\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "preds = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dmrc26xaxvf",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "mbbjc03m74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "                  Predicted\n",
      "                  Neg    Pos\n",
      "  Actual  Neg   [ 1758    327]\n",
      "          Pos   [  132    250]\n",
      "\n",
      "Performance Metrics:\n",
      "  - Accuracy:  0.8139 (81.39%)\n",
      "  - Precision: 0.4333 (43.33%)\n",
      "  - Recall:    0.6545 (65.45%)\n",
      "  - F1 Score:  0.5214 (52.14%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance metrics\n",
    "accuracy = sum(p == t for p, t in zip(preds, y_test)) / len(y_test)\n",
    "\n",
    "# Calculate confusion matrix elements\n",
    "true_positive = sum(1 for p, t in zip(preds, y_test) if p == 1 and t == 1)\n",
    "true_negative = sum(1 for p, t in zip(preds, y_test) if p == 0 and t == 0)\n",
    "false_positive = sum(1 for p, t in zip(preds, y_test) if p == 1 and t == 0)\n",
    "false_negative = sum(1 for p, t in zip(preds, y_test) if p == 0 and t == 1)\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"MODEL EVALUATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                  Predicted\")\n",
    "print(f\"                  Neg    Pos\")\n",
    "print(f\"  Actual  Neg   [{true_negative:5d}  {false_positive:5d}]\")\n",
    "print(f\"          Pos   [{false_negative:5d}  {true_positive:5d}]\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  - Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  - Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"  - Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"  - F1 Score:  {f1_score:.4f} ({f1_score*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oivnvzs6ur8",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3epfshn6f2g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix Heatmap (text-based):\n",
      "============================================================\n",
      "\n",
      "True Negatives:   1758 (71.26%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "True Positives:    250 (10.13%) ‚ñà‚ñà‚ñà‚ñà\n",
      "False Negatives:   132 ( 5.35%) ‚ñà‚ñà\n",
      "False Positives:   327 (13.25%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "Correct Predictions: 2008 / 2467 = 81.39%\n",
      "Incorrect Predictions: 459 / 2467 = 18.61%\n"
     ]
    }
   ],
   "source": [
    "# Create a visual representation of the confusion matrix\n",
    "print(\"\\nConfusion Matrix Heatmap (text-based):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate percentages\n",
    "total = true_negative + true_positive + false_negative + false_positive\n",
    "tn_pct = (true_negative / total) * 100\n",
    "tp_pct = (true_positive / total) * 100\n",
    "fn_pct = (false_negative / total) * 100\n",
    "fp_pct = (false_positive / total) * 100\n",
    "\n",
    "# Create visual bars\n",
    "def make_bar(value, max_val=total):\n",
    "    bar_length = int((value / max_val) * 40)\n",
    "    return \"‚ñà\" * bar_length\n",
    "\n",
    "print(f\"\\nTrue Negatives:  {true_negative:5d} ({tn_pct:5.2f}%) {make_bar(true_negative)}\")\n",
    "print(f\"True Positives:  {true_positive:5d} ({tp_pct:5.2f}%) {make_bar(true_positive)}\")\n",
    "print(f\"False Negatives: {false_negative:5d} ({fn_pct:5.2f}%) {make_bar(false_negative)}\")\n",
    "print(f\"False Positives: {false_positive:5d} ({fp_pct:5.2f}%) {make_bar(false_positive)}\")\n",
    "\n",
    "print(f\"\\nCorrect Predictions: {true_negative + true_positive} / {total} = {((true_negative + true_positive)/total)*100:.2f}%\")\n",
    "print(f\"Incorrect Predictions: {false_negative + false_positive} / {total} = {((false_negative + false_positive)/total)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etud8f7w1es",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bgp0xsh4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL COMPARISON (Test Set Accuracy)\n",
      "============================================================\n",
      "\n",
      "1. XGBoost (tuned)           0.8994 (89.94%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "2. Random Forest (tuned)     0.8946 (89.46%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "3. Neural Network            0.8881 (88.81%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "4. Logistic Regression       0.8731 (87.31%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "5. Naive Bayes               0.8139 (81.39%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚Üê YOUR MODEL\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "  - Best performing model: XGBoost (tuned) (0.8994)\n",
      "  - Naive Bayes score: 0.8139\n",
      "  - Performance gap: 0.0855 (8.55%)\n",
      "\n",
      "  Note: Naive Bayes provides a lightweight, interpretable baseline\n",
      "  with reasonable performance despite using simple probabilistic assumptions.\n"
     ]
    }
   ],
   "source": [
    "# Compare with other baseline models\n",
    "comparison = {\n",
    "    \"Logistic Regression\": 0.8731,\n",
    "    \"Neural Network\": 0.8881,\n",
    "    \"Random Forest (tuned)\": 0.8946,\n",
    "    \"XGBoost (tuned)\": 0.8994,\n",
    "    \"Naive Bayes\": round(accuracy, 4),\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODEL COMPARISON (Test Set Accuracy)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Sort by accuracy\n",
    "sorted_models = sorted(comparison.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (name, score) in enumerate(sorted_models, 1):\n",
    "    bar = \"‚ñà\" * int(score * 50)\n",
    "    marker = \" ‚Üê YOUR MODEL\" if name == \"Naive Bayes\" else \"\"\n",
    "    print(f\"{rank}. {name:<25} {score:.4f} ({score*100:.2f}%) {bar}{marker}\")\n",
    "\n",
    "# Calculate performance gap\n",
    "best_model = sorted_models[0][0]\n",
    "best_score = sorted_models[0][1]\n",
    "nb_score = comparison[\"Naive Bayes\"]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PERFORMANCE ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Best performing model: {best_model} ({best_score:.4f})\")\n",
    "print(f\"  - Naive Bayes score: {nb_score:.4f}\")\n",
    "print(f\"  - Performance gap: {abs(best_score - nb_score):.4f} ({abs(best_score - nb_score)*100:.2f}%)\")\n",
    "print(f\"\\n  Note: Naive Bayes provides a lightweight, interpretable baseline\")\n",
    "print(f\"  with reasonable performance despite using simple probabilistic assumptions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jyxyfihj9fg",
   "metadata": {},
   "source": [
    "## 9. Summary & Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6m82s11r7r8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "                    SUMMARY REPORT\n",
      "============================================================\n",
      "\n",
      "üìä Dataset:\n",
      "   - Total samples: 12330\n",
      "   - Train samples: 9863\n",
      "   - Test samples: 2467\n",
      "   - Features: 17 (10 numerical, 7 categorical)\n",
      "\n",
      "üéØ Model Performance:\n",
      "   - Accuracy:  0.8139\n",
      "   - Precision: 0.4333\n",
      "   - Recall:    0.6545\n",
      "   - F1 Score:  0.5214\n",
      "\n",
      "üîç Key Insights:\n",
      "   - The model correctly classified 2008 out of 2467 samples\n",
      "   - True Positive Rate (Sensitivity): 65.45%\n",
      "   - True Negative Rate (Specificity): 84.32%\n",
      "   - Positive Predictive Value: 43.33%\n",
      "\n",
      "‚úÖ Strengths:\n",
      "   - Pure Python implementation (no external ML libraries)\n",
      "   - Handles mixed numerical and categorical features\n",
      "   - Fast training and prediction\n",
      "   - Interpretable probabilistic model\n",
      "\n",
      "‚ö†Ô∏è  Limitations:\n",
      "   - Assumes feature independence (often violated in real data)\n",
      "   - Gaussian assumption for numerical features may not always hold\n",
      "   - Performance gap of 8.55% compared to best model\n",
      "\n",
      "============================================================\n",
      "‚úì Analysis Complete!\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" \"*20 + \"SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Dataset:\")\n",
    "print(f\"   - Total samples: {len(X)}\")\n",
    "print(f\"   - Train samples: {len(X_train)}\")\n",
    "print(f\"   - Test samples: {len(X_test)}\")\n",
    "print(f\"   - Features: {len(columns)} ({len(numeric_idx)} numerical, {len(categorical_idx)} categorical)\")\n",
    "\n",
    "print(\"\\nüéØ Model Performance:\")\n",
    "print(f\"   - Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   - Precision: {precision:.4f}\")\n",
    "print(f\"   - Recall:    {recall:.4f}\")\n",
    "print(f\"   - F1 Score:  {f1_score:.4f}\")\n",
    "\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(f\"   - The model correctly classified {true_negative + true_positive} out of {total} samples\")\n",
    "print(f\"   - True Positive Rate (Sensitivity): {recall:.2%}\")\n",
    "print(f\"   - True Negative Rate (Specificity): {true_negative/(true_negative+false_positive):.2%}\")\n",
    "print(f\"   - Positive Predictive Value: {precision:.2%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Strengths:\")\n",
    "print(\"   - Pure Python implementation (no external ML libraries)\")\n",
    "print(\"   - Handles mixed numerical and categorical features\")\n",
    "print(\"   - Fast training and prediction\")\n",
    "print(\"   - Interpretable probabilistic model\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Limitations:\")\n",
    "print(\"   - Assumes feature independence (often violated in real data)\")\n",
    "print(\"   - Gaussian assumption for numerical features may not always hold\")\n",
    "print(f\"   - Performance gap of {abs(best_score - nb_score)*100:.2f}% compared to best model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Analysis Complete!\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dt55f0h8u5k",
   "metadata": {},
   "source": [
    "# Parameter Tuning & Optimization\n",
    "\n",
    "Let's experiment with different parameters to improve the model's accuracy beyond 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7mga3bp4cb7",
   "metadata": {},
   "source": [
    "## 10. Enhanced Naive Bayes with Tunable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7yuv8audgdp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced NaiveBayesTuned class defined\n",
      "  - Tunable parameters: smoothing, min_variance\n",
      "  - Added predict_proba() method for ROC curves\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayesTuned:\n",
    "    \"\"\"Enhanced Naive Bayes classifier with tunable parameters.\"\"\"\n",
    "    \n",
    "    def __init__(self, numeric_idx: List[int], categorical_idx: List[int], \n",
    "                 smoothing: float = 1.0, min_variance: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize with tunable parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - smoothing: Laplace smoothing parameter for categorical features (default: 1.0)\n",
    "        - min_variance: Minimum variance threshold for numerical features (default: 1e-6)\n",
    "        \"\"\"\n",
    "        self.numeric_idx = numeric_idx\n",
    "        self.categorical_idx = categorical_idx\n",
    "        self.smoothing = smoothing  # Tunable parameter\n",
    "        self.min_variance = min_variance  # Tunable parameter\n",
    "        self.priors: Dict[int, float] = {}\n",
    "        self.num_stats: Dict[int, Dict[int, Tuple[float, float]]] = {}\n",
    "        self.cat_counts: Dict[int, Dict[int, Dict[str, int]]] = {}\n",
    "        self.cat_totals: Dict[int, Dict[int, int]] = {}\n",
    "    \n",
    "    def fit(self, X: List[List], y: List[int]):\n",
    "        \"\"\"Train the Naive Bayes model on the training data.\"\"\"\n",
    "        n = len(y)\n",
    "        class_indices: Dict[int, List[int]] = {0: [], 1: []}\n",
    "        for idx, label in enumerate(y):\n",
    "            class_indices[label].append(idx)\n",
    "        \n",
    "        for cls, indices in class_indices.items():\n",
    "            # Calculate prior probability for each class\n",
    "            self.priors[cls] = len(indices) / n\n",
    "            \n",
    "            # Calculate mean and variance for numerical features\n",
    "            self.num_stats[cls] = {}\n",
    "            for col in self.numeric_idx:\n",
    "                values = [X[i][col] for i in indices]\n",
    "                mean = sum(values) / len(values)\n",
    "                var = sum((v - mean) ** 2 for v in values) / max(len(values) - 1, 1)\n",
    "                self.num_stats[cls][col] = (mean, max(var, self.min_variance))\n",
    "            \n",
    "            # Count categorical feature values\n",
    "            self.cat_counts.setdefault(cls, {})\n",
    "            self.cat_totals.setdefault(cls, {})\n",
    "            for col in self.categorical_idx:\n",
    "                counts: Dict[str, int] = {}\n",
    "                for i in indices:\n",
    "                    key = X[i][col]\n",
    "                    counts[key] = counts.get(key, 0) + 1\n",
    "                self.cat_counts[cls][col] = counts\n",
    "                self.cat_totals[cls][col] = sum(counts.values())\n",
    "    \n",
    "    def _gaussian_log_prob(self, value: float, mean: float, var: float) -> float:\n",
    "        \"\"\"Calculate log probability for a Gaussian distribution.\"\"\"\n",
    "        return -0.5 * (\n",
    "            math.log(2 * math.pi * var) + ((value - mean) ** 2) / var\n",
    "        )\n",
    "    \n",
    "    def predict_proba_row(self, row: List) -> Tuple[float, float]:\n",
    "        \"\"\"Predict probability scores for a single row.\"\"\"\n",
    "        log_posteriors = {}\n",
    "        for cls in self.priors:\n",
    "            log_prob = math.log(self.priors[cls])\n",
    "            \n",
    "            # Add log probabilities for numerical features\n",
    "            for col in self.numeric_idx:\n",
    "                mean, var = self.num_stats[cls][col]\n",
    "                log_prob += self._gaussian_log_prob(row[col], mean, var)\n",
    "            \n",
    "            # Add log probabilities for categorical features with smoothing\n",
    "            for col in self.categorical_idx:\n",
    "                counts = self.cat_counts[cls][col]\n",
    "                total = self.cat_totals[cls][col]\n",
    "                value = row[col]\n",
    "                count = counts.get(value, 0)\n",
    "                vocab_size = len(counts)\n",
    "                log_prob += math.log((count + self.smoothing) / (total + self.smoothing * vocab_size))\n",
    "            \n",
    "            log_posteriors[cls] = log_prob\n",
    "        \n",
    "        # Convert log probabilities to probabilities using exp and normalize\n",
    "        max_log_prob = max(log_posteriors.values())\n",
    "        probs = {cls: math.exp(log_prob - max_log_prob) for cls, log_prob in log_posteriors.items()}\n",
    "        total_prob = sum(probs.values())\n",
    "        normalized_probs = {cls: p / total_prob for cls, p in probs.items()}\n",
    "        \n",
    "        # Return probability for class 0 and class 1\n",
    "        return normalized_probs.get(0, 0.0), normalized_probs.get(1, 0.0)\n",
    "    \n",
    "    def predict_row(self, row: List) -> int:\n",
    "        \"\"\"Predict the class label for a single row.\"\"\"\n",
    "        prob_0, prob_1 = self.predict_proba_row(row)\n",
    "        return 1 if prob_1 > prob_0 else 0\n",
    "    \n",
    "    def predict(self, X: List[List]) -> List[int]:\n",
    "        \"\"\"Predict class labels for multiple rows.\"\"\"\n",
    "        return [self.predict_row(row) for row in X]\n",
    "    \n",
    "    def predict_proba(self, X: List[List]) -> List[Tuple[float, float]]:\n",
    "        \"\"\"Predict probability scores for multiple rows.\"\"\"\n",
    "        return [self.predict_proba_row(row) for row in X]\n",
    "\n",
    "print(\"‚úì Enhanced NaiveBayesTuned class defined\")\n",
    "print(f\"  - Tunable parameters: smoothing, min_variance\")\n",
    "print(f\"  - Added predict_proba() method for ROC curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yub6qksd6q",
   "metadata": {},
   "source": [
    "## 11. Grid Search for Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "evf2yf6nztc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PARAMETER GRID SEARCH\n",
      "============================================================\n",
      "\n",
      "Testing 6 smoothing values: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
      "Testing 4 variance thresholds: [1e-09, 1e-06, 0.001, 0.1]\n",
      "Total configurations: 24\n",
      "\n",
      "============================================================\n",
      "TESTING CONFIGURATIONS...\n",
      "============================================================\n",
      "Smoothing=   0.1, MinVar=1.0e-09 ‚Üí Accuracy: 0.8143 (81.43%)\n",
      "Smoothing=   0.1, MinVar=1.0e-06 ‚Üí Accuracy: 0.8143 (81.43%)\n",
      "Smoothing=   0.1, MinVar=1.0e-03 ‚Üí Accuracy: 0.8456 (84.56%)\n",
      "Smoothing=   0.1, MinVar=1.0e-01 ‚Üí Accuracy: 0.8468 (84.68%)\n",
      "Smoothing=   0.5, MinVar=1.0e-09 ‚Üí Accuracy: 0.8143 (81.43%)\n",
      "Smoothing=   0.5, MinVar=1.0e-06 ‚Üí Accuracy: 0.8143 (81.43%)\n",
      "Smoothing=   0.5, MinVar=1.0e-03 ‚Üí Accuracy: 0.8452 (84.52%)\n",
      "Smoothing=   0.5, MinVar=1.0e-01 ‚Üí Accuracy: 0.8468 (84.68%)\n",
      "Smoothing=   1.0, MinVar=1.0e-09 ‚Üí Accuracy: 0.8139 (81.39%)\n",
      "Smoothing=   1.0, MinVar=1.0e-06 ‚Üí Accuracy: 0.8139 (81.39%)\n",
      "Smoothing=   1.0, MinVar=1.0e-03 ‚Üí Accuracy: 0.8452 (84.52%)\n",
      "Smoothing=   1.0, MinVar=1.0e-01 ‚Üí Accuracy: 0.8468 (84.68%)\n",
      "Smoothing=   2.0, MinVar=1.0e-09 ‚Üí Accuracy: 0.8143 (81.43%)\n",
      "Smoothing=   2.0, MinVar=1.0e-06 ‚Üí Accuracy: 0.8143 (81.43%)\n",
      "Smoothing=   2.0, MinVar=1.0e-03 ‚Üí Accuracy: 0.8452 (84.52%)\n",
      "Smoothing=   2.0, MinVar=1.0e-01 ‚Üí Accuracy: 0.8468 (84.68%)\n",
      "Smoothing=   5.0, MinVar=1.0e-09 ‚Üí Accuracy: 0.8148 (81.48%)\n",
      "Smoothing=   5.0, MinVar=1.0e-06 ‚Üí Accuracy: 0.8148 (81.48%)\n",
      "Smoothing=   5.0, MinVar=1.0e-03 ‚Üí Accuracy: 0.8439 (84.39%)\n",
      "Smoothing=   5.0, MinVar=1.0e-01 ‚Üí Accuracy: 0.8468 (84.68%)\n",
      "Smoothing=  10.0, MinVar=1.0e-09 ‚Üí Accuracy: 0.8143 (81.43%)\n",
      "Smoothing=  10.0, MinVar=1.0e-06 ‚Üí Accuracy: 0.8143 (81.43%)\n",
      "Smoothing=  10.0, MinVar=1.0e-03 ‚Üí Accuracy: 0.8435 (84.35%)\n",
      "Smoothing=  10.0, MinVar=1.0e-01 ‚Üí Accuracy: 0.8472 (84.72%)\n",
      "\n",
      "============================================================\n",
      "‚úì Tested 24 configurations\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid to search\n",
    "smoothing_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "variance_values = [1e-9, 1e-6, 1e-3, 1e-1]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PARAMETER GRID SEARCH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTesting {len(smoothing_values)} smoothing values: {smoothing_values}\")\n",
    "print(f\"Testing {len(variance_values)} variance thresholds: {variance_values}\")\n",
    "print(f\"Total configurations: {len(smoothing_values) * len(variance_values)}\")\n",
    "\n",
    "# Store results with probability predictions for ROC curves\n",
    "results = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING CONFIGURATIONS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for smooth in smoothing_values:\n",
    "    for var_threshold in variance_values:\n",
    "        # Train model with these parameters\n",
    "        model = NaiveBayesTuned(\n",
    "            numeric_idx=numeric_idx, \n",
    "            categorical_idx=categorical_idx,\n",
    "            smoothing=smooth,\n",
    "            min_variance=var_threshold\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Get probability predictions for ROC curve\n",
    "        proba_predictions = model.predict_proba(X_test)\n",
    "        y_scores = [prob_1 for prob_0, prob_1 in proba_predictions]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = sum(p == t for p, t in zip(predictions, y_test)) / len(y_test)\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            'smoothing': smooth,\n",
    "            'min_variance': var_threshold,\n",
    "            'accuracy': acc,\n",
    "            'y_scores': y_scores,  # Store for ROC plotting\n",
    "            'model': model\n",
    "        })\n",
    "        \n",
    "        print(f\"Smoothing={smooth:6.1f}, MinVar={var_threshold:.1e} ‚Üí Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úì Tested {len(results)} configurations\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xa4z5tguh3i",
   "metadata": {},
   "source": [
    "## 12. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "v9au0zcyzgq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOP 10 PARAMETER CONFIGURATIONS\n",
      "============================================================\n",
      "\n",
      "Rank   Smoothing    MinVariance    Accuracy   Improvement\n",
      "------------------------------------------------------------\n",
      "1      10.0         1.0e-01        0.8472     +3.32%  ‚≠ê BEST\n",
      "2      0.1          1.0e-01        0.8468     +3.28%\n",
      "3      0.5          1.0e-01        0.8468     +3.28%\n",
      "4      1.0          1.0e-01        0.8468     +3.28%\n",
      "5      2.0          1.0e-01        0.8468     +3.28%\n",
      "6      5.0          1.0e-01        0.8468     +3.28%\n",
      "7      0.1          1.0e-03        0.8456     +3.16%\n",
      "8      0.5          1.0e-03        0.8452     +3.12%\n",
      "9      1.0          1.0e-03        0.8452     +3.12%\n",
      "10     2.0          1.0e-03        0.8452     +3.12%\n",
      "\n",
      "============================================================\n",
      "BEST CONFIGURATION FOUND\n",
      "============================================================\n",
      "  Smoothing parameter: 10.0\n",
      "  Min variance threshold: 1.0e-01\n",
      "  Test accuracy: 0.8472 (84.72%)\n",
      "  Improvement over baseline: +3.32%\n",
      "  Baseline accuracy: 0.8139 (81.39%)\n"
     ]
    }
   ],
   "source": [
    "# Sort results by accuracy\n",
    "sorted_results = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TOP 10 PARAMETER CONFIGURATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Rank':<6} {'Smoothing':<12} {'MinVariance':<14} {'Accuracy':<10} {'Improvement'}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "baseline_acc = accuracy  # Original accuracy\n",
    "\n",
    "for rank, result in enumerate(sorted_results[:10], 1):\n",
    "    smooth = result['smoothing']\n",
    "    var_th = result['min_variance']\n",
    "    acc = result['accuracy']\n",
    "    improvement = (acc - baseline_acc) * 100\n",
    "    \n",
    "    marker = \"  ‚≠ê BEST\" if rank == 1 else \"\"\n",
    "    print(f\"{rank:<6} {smooth:<12.1f} {var_th:<14.1e} {acc:.4f}     +{improvement:.2f}%{marker}\")\n",
    "\n",
    "# Get best configuration\n",
    "best = sorted_results[0]\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST CONFIGURATION FOUND\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Smoothing parameter: {best['smoothing']}\")\n",
    "print(f\"  Min variance threshold: {best['min_variance']:.1e}\")\n",
    "print(f\"  Test accuracy: {best['accuracy']:.4f} ({best['accuracy']*100:.2f}%)\")\n",
    "print(f\"  Improvement over baseline: +{(best['accuracy'] - baseline_acc)*100:.2f}%\")\n",
    "print(f\"  Baseline accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l96pwqefb3p",
   "metadata": {},
   "source": [
    "### Parameter Impact Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ku3w4l88vf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SMOOTHING PARAMETER IMPACT\n",
      "============================================================\n",
      "\n",
      "Average accuracy across all variance thresholds:\n",
      "\n",
      "  Smoothing=   0.1 ‚Üí 0.8303 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚≠ê\n",
      "  Smoothing=   0.5 ‚Üí 0.8302 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Smoothing=   1.0 ‚Üí 0.8300 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Smoothing=   2.0 ‚Üí 0.8302 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Smoothing=   5.0 ‚Üí 0.8301 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Smoothing=  10.0 ‚Üí 0.8299 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "============================================================\n",
      "VARIANCE THRESHOLD IMPACT\n",
      "============================================================\n",
      "\n",
      "Average accuracy across all smoothing values:\n",
      "\n",
      "  MinVar=1.0e-09 ‚Üí 0.8143 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  MinVar=1.0e-06 ‚Üí 0.8143 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  MinVar=1.0e-03 ‚Üí 0.8448 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  MinVar=1.0e-01 ‚Üí 0.8468 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚≠ê\n"
     ]
    }
   ],
   "source": [
    "# Analyze smoothing parameter impact\n",
    "print(\"=\"*60)\n",
    "print(\"SMOOTHING PARAMETER IMPACT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average accuracy for each smoothing value\n",
    "smoothing_impact = {}\n",
    "for smooth in smoothing_values:\n",
    "    accs = [r['accuracy'] for r in results if r['smoothing'] == smooth]\n",
    "    smoothing_impact[smooth] = sum(accs) / len(accs)\n",
    "\n",
    "print(\"\\nAverage accuracy across all variance thresholds:\\n\")\n",
    "max_acc = max(smoothing_impact.values())\n",
    "for smooth, avg_acc in sorted(smoothing_impact.items()):\n",
    "    bar = \"‚ñà\" * int(avg_acc * 50)\n",
    "    marker = \" ‚≠ê\" if avg_acc == max_acc else \"\"\n",
    "    print(f\"  Smoothing={smooth:6.1f} ‚Üí {avg_acc:.4f} {bar}{marker}\")\n",
    "\n",
    "# Analyze variance threshold impact\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VARIANCE THRESHOLD IMPACT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "variance_impact = {}\n",
    "for var_th in variance_values:\n",
    "    accs = [r['accuracy'] for r in results if r['min_variance'] == var_th]\n",
    "    variance_impact[var_th] = sum(accs) / len(accs)\n",
    "\n",
    "print(\"\\nAverage accuracy across all smoothing values:\\n\")\n",
    "max_acc = max(variance_impact.values())\n",
    "for var_th, avg_acc in sorted(variance_impact.items()):\n",
    "    bar = \"‚ñà\" * int(avg_acc * 50)\n",
    "    marker = \" ‚≠ê\" if avg_acc == max_acc else \"\"\n",
    "    print(f\"  MinVar={var_th:.1e} ‚Üí {avg_acc:.4f} {bar}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1q88jutrnnl",
   "metadata": {},
   "source": [
    "## 13. Final Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "otlekvkmhxp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING OPTIMIZED MODEL\n",
      "============================================================\n",
      "\n",
      "‚úì Using optimal parameters:\n",
      "  - Smoothing: 10.0\n",
      "  - Min Variance: 1.0e-01\n",
      "\n",
      "============================================================\n",
      "OPTIMIZED MODEL PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "                  Predicted\n",
      "                  Neg    Pos\n",
      "  Actual  Neg   [ 1914    171]\n",
      "          Pos   [  206    176]\n",
      "\n",
      "Performance Metrics:\n",
      "  - Accuracy:  0.8472 (84.72%)\n",
      "  - Precision: 0.5072 (50.72%)\n",
      "  - Recall:    0.4607 (46.07%)\n",
      "  - F1 Score:  0.4829 (48.29%)\n"
     ]
    }
   ],
   "source": [
    "# Train final model with best parameters\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING OPTIMIZED MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "optimized_model = NaiveBayesTuned(\n",
    "    numeric_idx=numeric_idx,\n",
    "    categorical_idx=categorical_idx,\n",
    "    smoothing=best['smoothing'],\n",
    "    min_variance=best['min_variance']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Using optimal parameters:\")\n",
    "print(f\"  - Smoothing: {best['smoothing']}\")\n",
    "print(f\"  - Min Variance: {best['min_variance']:.1e}\")\n",
    "\n",
    "optimized_model.fit(X_train, y_train)\n",
    "optimized_preds = optimized_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "opt_accuracy = sum(p == t for p, t in zip(optimized_preds, y_test)) / len(y_test)\n",
    "opt_tp = sum(1 for p, t in zip(optimized_preds, y_test) if p == 1 and t == 1)\n",
    "opt_tn = sum(1 for p, t in zip(optimized_preds, y_test) if p == 0 and t == 0)\n",
    "opt_fp = sum(1 for p, t in zip(optimized_preds, y_test) if p == 1 and t == 0)\n",
    "opt_fn = sum(1 for p, t in zip(optimized_preds, y_test) if p == 0 and t == 1)\n",
    "opt_precision = opt_tp / (opt_tp + opt_fp)\n",
    "opt_recall = opt_tp / (opt_tp + opt_fn)\n",
    "opt_f1 = 2 * (opt_precision * opt_recall) / (opt_precision + opt_recall)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZED MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                  Predicted\")\n",
    "print(f\"                  Neg    Pos\")\n",
    "print(f\"  Actual  Neg   [{opt_tn:5d}  {opt_fp:5d}]\")\n",
    "print(f\"          Pos   [{opt_fn:5d}  {opt_tp:5d}]\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  - Accuracy:  {opt_accuracy:.4f} ({opt_accuracy*100:.2f}%)\")\n",
    "print(f\"  - Precision: {opt_precision:.4f} ({opt_precision*100:.2f}%)\")\n",
    "print(f\"  - Recall:    {opt_recall:.4f} ({opt_recall*100:.2f}%)\")\n",
    "print(f\"  - F1 Score:  {opt_f1:.4f} ({opt_f1*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dskiddieg5o",
   "metadata": {},
   "source": [
    "## 14. Baseline vs Optimized Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6bnx0042jqe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE vs OPTIMIZED COMPARISON\n",
      "============================================================\n",
      "\n",
      "Metric          Baseline     Optimized    Improvement\n",
      "------------------------------------------------------------\n",
      "Accuracy        0.8139      0.8472       ‚Üë 3.32%\n",
      "Precision       0.4333      0.5072       ‚Üë 7.39%\n",
      "Recall          0.6545      0.4607       ‚Üì 19.37%\n",
      "F1 Score        0.5214      0.4829       ‚Üì 3.85%\n",
      "\n",
      "============================================================\n",
      "PARAMETER CHANGES\n",
      "============================================================\n",
      "Baseline  ‚Üí Smoothing: 1.0,   MinVar: 1.0e-06\n",
      "Optimized ‚Üí Smoothing: 10.0,   MinVar: 1.0e-01\n",
      "\n",
      "============================================================\n",
      "UPDATED MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "1. XGBoost (tuned)              0.8994 (89.94%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "2. Random Forest (tuned)        0.8946 (89.46%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "3. Neural Network               0.8881 (88.81%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "4. Logistic Regression          0.8731 (87.31%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "5. Naive Bayes (optimized)      0.8472 (84.72%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚≠ê OPTIMIZED\n",
      "6. Naive Bayes (baseline)       0.8139 (81.39%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚Üê BASELINE\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE GAP ANALYSIS\n",
      "============================================================\n",
      "Gap to best model (baseline):  0.0855 (8.55%)\n",
      "Gap to best model (optimized): 0.0522 (5.22%)\n",
      "Gap reduction: 3.32%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BASELINE vs OPTIMIZED COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_comparison = {\n",
    "    'Accuracy': (accuracy, opt_accuracy),\n",
    "    'Precision': (precision, opt_precision),\n",
    "    'Recall': (recall, opt_recall),\n",
    "    'F1 Score': (f1_score, opt_f1)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Metric':<15} {'Baseline':<12} {'Optimized':<12} {'Improvement'}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for metric_name, (baseline_val, optimized_val) in metrics_comparison.items():\n",
    "    improvement = (optimized_val - baseline_val) * 100\n",
    "    symbol = \"‚Üë\" if improvement > 0 else (\"‚Üì\" if improvement < 0 else \"‚Üí\")\n",
    "    print(f\"{metric_name:<15} {baseline_val:.4f}      {optimized_val:.4f}       {symbol} {abs(improvement):.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER CHANGES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline  ‚Üí Smoothing: 1.0,   MinVar: 1.0e-06\")\n",
    "print(f\"Optimized ‚Üí Smoothing: {best['smoothing']},   MinVar: {best['min_variance']:.1e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPDATED MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Update comparison with optimized Naive Bayes\n",
    "final_comparison = {\n",
    "    \"Logistic Regression\": 0.8731,\n",
    "    \"Neural Network\": 0.8881,\n",
    "    \"Random Forest (tuned)\": 0.8946,\n",
    "    \"XGBoost (tuned)\": 0.8994,\n",
    "    \"Naive Bayes (baseline)\": round(accuracy, 4),\n",
    "    \"Naive Bayes (optimized)\": round(opt_accuracy, 4),\n",
    "}\n",
    "\n",
    "sorted_final = sorted(final_comparison.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print()\n",
    "for rank, (name, score) in enumerate(sorted_final, 1):\n",
    "    bar = \"‚ñà\" * int(score * 50)\n",
    "    marker = \" ‚≠ê OPTIMIZED\" if \"optimized\" in name else \"\"\n",
    "    marker = marker or (\" ‚Üê BASELINE\" if \"baseline\" in name else \"\")\n",
    "    print(f\"{rank}. {name:<28} {score:.4f} ({score*100:.2f}%) {bar}{marker}\")\n",
    "\n",
    "# Calculate new gap\n",
    "best_overall = sorted_final[0][1]\n",
    "gap_baseline = abs(best_overall - accuracy)\n",
    "gap_optimized = abs(best_overall - opt_accuracy)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PERFORMANCE GAP ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Gap to best model (baseline):  {gap_baseline:.4f} ({gap_baseline*100:.2f}%)\")\n",
    "print(f\"Gap to best model (optimized): {gap_optimized:.4f} ({gap_optimized*100:.2f}%)\")\n",
    "print(f\"Gap reduction: {(gap_baseline - gap_optimized)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxzfpsiaqgb",
   "metadata": {},
   "source": [
    "# ROC Curve Analysis\n",
    "\n",
    "Receiver Operating Characteristic (ROC) curves visualize the trade-off between True Positive Rate and False Positive Rate at different classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txvfu2vgsxq",
   "metadata": {},
   "source": [
    "## 15. ROC Curve Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "wqzlzv45de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ROC curve computation function defined\n",
      "  - Computes FPR, TPR at all thresholds\n",
      "  - Calculates AUC using trapezoidal rule\n"
     ]
    }
   ],
   "source": [
    "def compute_roc_curve(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Compute ROC curve points (FPR, TPR) and AUC.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True labels (0 or 1)\n",
    "    - y_scores: Predicted probability scores for positive class\n",
    "    \n",
    "    Returns:\n",
    "    - fpr_list: False Positive Rates\n",
    "    - tpr_list: True Positive Rates\n",
    "    - auc: Area Under the Curve\n",
    "    \"\"\"\n",
    "    # Create list of (score, true_label) pairs and sort by score descending\n",
    "    pairs = list(zip(y_scores, y_true))\n",
    "    pairs.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    # Count positives and negatives\n",
    "    n_pos = sum(y_true)\n",
    "    n_neg = len(y_true) - n_pos\n",
    "    \n",
    "    # Initialize\n",
    "    fpr_list = [0.0]\n",
    "    tpr_list = [0.0]\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    prev_score = float('inf')\n",
    "    \n",
    "    # Compute ROC points\n",
    "    for score, label in pairs:\n",
    "        if score != prev_score:\n",
    "            # Add point\n",
    "            fpr = fp / n_neg if n_neg > 0 else 0.0\n",
    "            tpr = tp / n_pos if n_pos > 0 else 0.0\n",
    "            fpr_list.append(fpr)\n",
    "            tpr_list.append(tpr)\n",
    "            prev_score = score\n",
    "        \n",
    "        if label == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    # Add final point (1, 1)\n",
    "    fpr_list.append(1.0)\n",
    "    tpr_list.append(1.0)\n",
    "    \n",
    "    # Calculate AUC using trapezoidal rule\n",
    "    auc = 0.0\n",
    "    for i in range(1, len(fpr_list)):\n",
    "        auc += (fpr_list[i] - fpr_list[i-1]) * (tpr_list[i] + tpr_list[i-1]) / 2.0\n",
    "    \n",
    "    return fpr_list, tpr_list, auc\n",
    "\n",
    "print(\"‚úì ROC curve computation function defined\")\n",
    "print(\"  - Computes FPR, TPR at all thresholds\")\n",
    "print(\"  - Calculates AUC using trapezoidal rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20yg6d2x1f",
   "metadata": {},
   "source": [
    "## 16. ROC Curves for Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n5wqc51jmh",
   "metadata": {},
   "outputs": [],
   "source": "# Plot ROC curves for top 5 models\nprint(\"=\"*60)\nprint(\"COMPUTING ROC CURVES FOR TOP 5 MODELS\")\nprint(\"=\"*60)\n\n# Sort results by accuracy and take top 5\ntop_5_results = sorted(results, key=lambda x: x['accuracy'], reverse=True)[:5]\n\nplt.figure(figsize=(10, 8))\n\n# Plot diagonal reference line (random classifier)\nplt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC=0.50)', alpha=0.5)\n\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n\nprint(\"\\nModel Configuration | AUC Score | Accuracy\")\nprint(\"-\" * 60)\n\nfor idx, result in enumerate(top_5_results):\n    # Compute ROC curve\n    fpr, tpr, auc = compute_roc_curve(y_test, result['y_scores'])\n    \n    # Create label\n    label = f\"S={result['smoothing']:.1f}, V={result['min_variance']:.0e} (AUC={auc:.3f})\"\n    \n    # Plot\n    plt.plot(fpr, tpr, linewidth=2.5, color=colors[idx], label=label, alpha=0.8)\n    \n    # Print info\n    print(f\"Rank {idx+1}: Smoothing={result['smoothing']:5.1f}, MinVar={result['min_variance']:.1e} | \"\n          f\"AUC={auc:.4f} | Acc={result['accuracy']:.4f}\")\n\nplt.xlabel('False Positive Rate (FPR)', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate (TPR)', fontsize=12, fontweight='bold')\nplt.title('ROC Curves - Top 5 Parameter Configurations', fontsize=14, fontweight='bold', pad=20)\nplt.legend(loc='lower right', fontsize=10)\nplt.grid(True, alpha=0.3, linestyle='--')\nplt.xlim([-0.02, 1.02])\nplt.ylim([-0.02, 1.02])\n\n# Add text annotation\nplt.text(0.98, 0.02, 'Better ‚Üí', ha='right', va='bottom', fontsize=10, \n         style='italic', alpha=0.7)\nplt.text(0.02, 0.98, '‚Üê Better', ha='left', va='top', fontsize=10, \n         style='italic', alpha=0.7)\n\nplt.tight_layout()\n\n# Save the plot\noutput_path = Path.home() / \"Documents\" / \"OnlinePurchaseRetentionSTAT441\" / \"naive_bayes_plots\" / \"roc_curve_top5.png\"\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\nprint(f\"\\n‚úì Plot saved to: {output_path}\")\n\nplt.show()\n\nprint(\"\\n‚úì ROC curves plotted for top 5 configurations\")"
  },
  {
   "cell_type": "markdown",
   "id": "s68wf984a7q",
   "metadata": {},
   "source": [
    "## 17. ROC Curves by Parameter Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7iqle9f0t",
   "metadata": {},
   "outputs": [],
   "source": "# Create side-by-side ROC plots for parameter analysis\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\n# LEFT PLOT: Effect of Smoothing Parameter (fix variance at 1e-6)\nax1 = axes[0]\nax1.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.3)\n\nfixed_variance = 1e-6\nsmoothing_colors = plt.cm.viridis([i / len(smoothing_values) for i in range(len(smoothing_values))])\n\nprint(\"=\"*60)\nprint(\"SMOOTHING PARAMETER EFFECT (Variance=1e-6)\")\nprint(\"=\"*60)\nprint(f\"{'Smoothing':<12} {'AUC':<10} {'Accuracy'}\")\nprint(\"-\"*60)\n\nfor idx, smooth in enumerate(smoothing_values):\n    # Find result with this smoothing and fixed variance\n    result = [r for r in results if r['smoothing'] == smooth and r['min_variance'] == fixed_variance][0]\n    \n    # Compute ROC\n    fpr, tpr, auc = compute_roc_curve(y_test, result['y_scores'])\n    \n    # Plot\n    ax1.plot(fpr, tpr, linewidth=2.5, color=smoothing_colors[idx], \n             label=f'S={smooth} (AUC={auc:.3f})', alpha=0.8)\n    \n    print(f\"{smooth:<12.1f} {auc:<10.4f} {result['accuracy']:.4f}\")\n\nax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\nax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\nax1.set_title('Impact of Smoothing Parameter\\n(Min Variance = 1e-6)', fontsize=12, fontweight='bold')\nax1.legend(loc='lower right', fontsize=9)\nax1.grid(True, alpha=0.3, linestyle='--')\nax1.set_xlim([-0.02, 1.02])\nax1.set_ylim([-0.02, 1.02])\n\n# RIGHT PLOT: Effect of Variance Threshold (fix smoothing at 1.0)\nax2 = axes[1]\nax2.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.3)\n\nfixed_smoothing = 1.0\nvariance_colors = plt.cm.plasma([i / len(variance_values) for i in range(len(variance_values))])\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VARIANCE THRESHOLD EFFECT (Smoothing=1.0)\")\nprint(\"=\"*60)\nprint(f\"{'MinVariance':<12} {'AUC':<10} {'Accuracy'}\")\nprint(\"-\"*60)\n\nfor idx, var_th in enumerate(variance_values):\n    # Find result with this variance and fixed smoothing\n    result = [r for r in results if r['min_variance'] == var_th and r['smoothing'] == fixed_smoothing][0]\n    \n    # Compute ROC\n    fpr, tpr, auc = compute_roc_curve(y_test, result['y_scores'])\n    \n    # Plot\n    ax2.plot(fpr, tpr, linewidth=2.5, color=variance_colors[idx], \n             label=f'V={var_th:.0e} (AUC={auc:.3f})', alpha=0.8)\n    \n    print(f\"{var_th:<12.1e} {auc:<10.4f} {result['accuracy']:.4f}\")\n\nax2.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\nax2.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\nax2.set_title('Impact of Variance Threshold\\n(Smoothing = 1.0)', fontsize=12, fontweight='bold')\nax2.legend(loc='lower right', fontsize=9)\nax2.grid(True, alpha=0.3, linestyle='--')\nax2.set_xlim([-0.02, 1.02])\nax2.set_ylim([-0.02, 1.02])\n\nplt.tight_layout()\n\n# Save the plot\noutput_path = Path.home() / \"Documents\" / \"OnlinePurchaseRetentionSTAT441\" / \"naive_bayes_plots\" / \"roc_parameter_impact.png\"\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\nprint(f\"\\n‚úì Plot saved to: {output_path}\")\n\nplt.show()\n\nprint(\"\\n‚úì Parameter impact ROC curves plotted\")"
  },
  {
   "cell_type": "markdown",
   "id": "6xnnmu4rqze",
   "metadata": {},
   "source": [
    "## 18. Complete ROC Grid - All Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zgyniyesu6",
   "metadata": {},
   "outputs": [],
   "source": "# Create a grid showing all 24 configurations\nfig, axes = plt.subplots(len(variance_values), len(smoothing_values), \n                         figsize=(18, 12), sharex=True, sharey=True)\n\nprint(\"=\"*60)\nprint(\"ALL 24 CONFIGURATION ROC CURVES\")\nprint(\"=\"*60)\nprint(\"\\nGrid Layout: Rows=Variance Threshold, Columns=Smoothing\")\nprint()\n\n# Add a main title\nfig.suptitle('ROC Curves for All Parameter Configurations\\n(Rows: Min Variance, Columns: Smoothing)', \n             fontsize=16, fontweight='bold', y=0.995)\n\nfor row_idx, var_th in enumerate(variance_values):\n    for col_idx, smooth in enumerate(smoothing_values):\n        ax = axes[row_idx, col_idx]\n        \n        # Find result\n        result = [r for r in results if r['smoothing'] == smooth and r['min_variance'] == var_th][0]\n        \n        # Compute ROC\n        fpr, tpr, auc = compute_roc_curve(y_test, result['y_scores'])\n        \n        # Plot diagonal\n        ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.3)\n        \n        # Plot ROC curve\n        # Color based on AUC score\n        if auc >= 0.85:\n            color = '#2ecc71'  # Green for high AUC\n        elif auc >= 0.80:\n            color = '#3498db'  # Blue for medium AUC\n        else:\n            color = '#e74c3c'  # Red for low AUC\n        \n        ax.plot(fpr, tpr, linewidth=2, color=color, alpha=0.8)\n        ax.fill_between(fpr, tpr, alpha=0.2, color=color)\n        \n        # Add title with parameters and AUC\n        title = f'S={smooth:.1f}, V={var_th:.0e}\\nAUC={auc:.3f}, Acc={result[\"accuracy\"]:.3f}'\n        ax.set_title(title, fontsize=8, pad=3)\n        \n        # Grid\n        ax.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)\n        \n        # Labels only on edges\n        if row_idx == len(variance_values) - 1:\n            ax.set_xlabel('FPR', fontsize=9)\n        if col_idx == 0:\n            ax.set_ylabel('TPR', fontsize=9)\n        \n        # Set limits\n        ax.set_xlim([-0.02, 1.02])\n        ax.set_ylim([-0.02, 1.02])\n        \n        # Tick settings\n        ax.tick_params(labelsize=7)\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.96, hspace=0.3, wspace=0.1)\n\n# Save the plot\noutput_path = Path.home() / \"Documents\" / \"OnlinePurchaseRetentionSTAT441\" / \"naive_bayes_plots\" / \"roc_grid_all_configs.png\"\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\nprint(f\"\\n‚úì Plot saved to: {output_path}\")\n\nplt.show()\n\nprint(\"‚úì Complete ROC grid plotted\")\nprint(\"\\nColor coding:\")\nprint(\"  üü¢ Green:  AUC ‚â• 0.85 (Excellent)\")\nprint(\"  üîµ Blue:   0.80 ‚â§ AUC < 0.85 (Good)\")\nprint(\"  üî¥ Red:    AUC < 0.80 (Needs improvement)\")"
  },
  {
   "cell_type": "markdown",
   "id": "u7k6o6oap6d",
   "metadata": {},
   "source": [
    "## 19. AUC Score Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ccyxrl93cs5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AUC SCORE MATRIX\n",
      "================================================================================\n",
      "\n",
      "Rows: Min Variance Threshold | Columns: Smoothing Parameter\n",
      "\n",
      "MinVar \\ Smooth |    0.1 |    0.5 |    1.0 |    2.0 |    5.0 |   10.0 |\n",
      "--------------------------------------------------------------------------------\n",
      "  1.0e-09       | 0.8278 ‚Ä¢| 0.8277 ‚Ä¢| 0.8276 ‚Ä¢| 0.8275 ‚Ä¢| 0.8272 ‚Ä¢| 0.8270 ‚Ä¢|\n",
      "  1.0e-06       | 0.8278 ‚Ä¢| 0.8277 ‚Ä¢| 0.8276 ‚Ä¢| 0.8275 ‚Ä¢| 0.8272 ‚Ä¢| 0.8270 ‚Ä¢|\n",
      "  1.0e-03       | 0.8330 ‚Ä¢| 0.8327 ‚Ä¢| 0.8326 ‚Ä¢| 0.8325 ‚Ä¢| 0.8322 ‚Ä¢| 0.8316 ‚Ä¢|\n",
      "  1.0e-01       | 0.8376 ‚Ä¢| 0.8374 ‚Ä¢| 0.8373 ‚Ä¢| 0.8371 ‚Ä¢| 0.8367 ‚Ä¢| 0.8360 ‚Ä¢|\n",
      "\n",
      "================================================================================\n",
      "LEGEND\n",
      "================================================================================\n",
      "‚≠ê : AUC ‚â• 0.85 (Excellent)\n",
      "‚Ä¢ : 0.80 ‚â§ AUC < 0.85 (Good)\n",
      "  : AUC < 0.80 (Needs improvement)\n",
      "\n",
      "üèÜ BEST AUC SCORE: 0.8376\n",
      "   Parameters: Smoothing=0.1, MinVar=1.0e-01\n"
     ]
    }
   ],
   "source": [
    "# Create AUC score table\n",
    "print(\"=\"*80)\n",
    "print(\"AUC SCORE MATRIX\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRows: Min Variance Threshold | Columns: Smoothing Parameter\")\n",
    "print()\n",
    "\n",
    "# Compute all AUC scores\n",
    "auc_matrix = []\n",
    "for var_th in variance_values:\n",
    "    row = []\n",
    "    for smooth in smoothing_values:\n",
    "        result = [r for r in results if r['smoothing'] == smooth and r['min_variance'] == var_th][0]\n",
    "        fpr, tpr, auc = compute_roc_curve(y_test, result['y_scores'])\n",
    "        row.append(auc)\n",
    "    auc_matrix.append(row)\n",
    "\n",
    "# Print header\n",
    "header = \"MinVar \\\\ Smooth |\"\n",
    "for smooth in smoothing_values:\n",
    "    header += f\" {smooth:6.1f} |\"\n",
    "print(header)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Print rows\n",
    "for var_idx, var_th in enumerate(variance_values):\n",
    "    row_str = f\"  {var_th:.1e}       |\"\n",
    "    for smooth_idx in range(len(smoothing_values)):\n",
    "        auc_val = auc_matrix[var_idx][smooth_idx]\n",
    "        # Add color indicator\n",
    "        if auc_val >= 0.85:\n",
    "            marker = \"‚≠ê\"\n",
    "        elif auc_val >= 0.80:\n",
    "            marker = \" ‚Ä¢\"\n",
    "        else:\n",
    "            marker = \"  \"\n",
    "        row_str += f\" {auc_val:.4f}{marker}|\"\n",
    "    print(row_str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LEGEND\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚≠ê : AUC ‚â• 0.85 (Excellent)\")\n",
    "print(\"‚Ä¢ : 0.80 ‚â§ AUC < 0.85 (Good)\")\n",
    "print(\"  : AUC < 0.80 (Needs improvement)\")\n",
    "\n",
    "# Find best AUC\n",
    "best_auc = max(max(row) for row in auc_matrix)\n",
    "best_indices = [(i, j) for i, row in enumerate(auc_matrix) for j, val in enumerate(row) if val == best_auc]\n",
    "best_var_idx, best_smooth_idx = best_indices[0]\n",
    "\n",
    "print(f\"\\nüèÜ BEST AUC SCORE: {best_auc:.4f}\")\n",
    "print(f\"   Parameters: Smoothing={smoothing_values[best_smooth_idx]}, MinVar={variance_values[best_var_idx]:.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oa7ns1ghcdg",
   "source": "# Log Transformation Experiment\n\nMany numerical features have skewed distributions. Let's apply log transformation to make them more Gaussian, which better fits Naive Bayes assumptions.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "l99hyyrum5",
   "source": "## 24. Apply Log Transformation to Numerical Features\n\nTransform skewed numerical features using log(x + 1) to make distributions more Gaussian.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tm7ay88q4n",
   "source": "# Apply log transformation to numerical features\nimport numpy as np\n\nprint(\"=\"*60)\nprint(\"LOG TRANSFORMATION OF NUMERICAL FEATURES\")\nprint(\"=\"*60)\n\n# Create transformed versions of train and test data\nX_train_log = [row.copy() for row in X_train]\nX_test_log = [row.copy() for row in X_test]\n\n# Apply log(x + 1) to numerical features\n# Adding 1 handles zero values, and log is monotonic so relative ordering preserved\nfor i in range(len(X_train_log)):\n    for col_idx in numeric_idx:\n        X_train_log[i][col_idx] = np.log1p(X_train_log[i][col_idx])  # log1p(x) = log(1+x)\n\nfor i in range(len(X_test_log)):\n    for col_idx in numeric_idx:\n        X_test_log[i][col_idx] = np.log1p(X_test_log[i][col_idx])\n\nprint(f\"\\n‚úì Applied log(x + 1) transformation to {len(numeric_idx)} numerical features\")\nprint(f\"  - Training samples: {len(X_train_log)}\")\nprint(f\"  - Test samples: {len(X_test_log)}\")\n\n# Show example transformation\nprint(f\"\\n{'Feature':<30} {'Original':<15} {'Log-Transformed'}\")\nprint(\"-\"*60)\nsample_idx = 0\nfor col_idx in numeric_idx[:5]:  # Show first 5 features\n    original_val = X_train[sample_idx][col_idx]\n    transformed_val = X_train_log[sample_idx][col_idx]\n    print(f\"{columns[col_idx]:<30} {original_val:<15.3f} {transformed_val:.3f}\")\n\nprint(\"\\n\" + \"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4qk6gdt213j",
   "source": "## 25. Train Naive Bayes with Log-Transformed Features\n\nTrain models with the same parameter grid but using log-transformed numerical features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bimmc1ya1qf",
   "source": "# Train models with log-transformed features\nprint(\"=\"*60)\nprint(\"TRAINING WITH LOG-TRANSFORMED FEATURES\")\nprint(\"=\"*60)\n\n# Test same parameter grid\nresults_log = []\n\nprint(f\"\\nTesting {len(smoothing_values)} √ó {len(variance_values)} = {len(smoothing_values) * len(variance_values)} configurations...\")\nprint()\n\nfor smooth in smoothing_values:\n    for var_threshold in variance_values:\n        # Train model with log-transformed data\n        model_log = NaiveBayesTuned(\n            numeric_idx=numeric_idx, \n            categorical_idx=categorical_idx,\n            smoothing=smooth,\n            min_variance=var_threshold\n        )\n        model_log.fit(X_train_log, y_train)\n        \n        # Make predictions\n        predictions_log = model_log.predict(X_test_log)\n        \n        # Get probability predictions for ROC curve\n        proba_predictions_log = model_log.predict_proba(X_test_log)\n        y_scores_log = [prob_1 for prob_0, prob_1 in proba_predictions_log]\n        \n        # Calculate accuracy\n        acc_log = sum(p == t for p, t in zip(predictions_log, y_test)) / len(y_test)\n        \n        # Store result\n        results_log.append({\n            'smoothing': smooth,\n            'min_variance': var_threshold,\n            'accuracy': acc_log,\n            'y_scores': y_scores_log,\n            'model': model_log\n        })\n        \n        print(f\"Smoothing={smooth:6.1f}, MinVar={var_threshold:.1e} ‚Üí Accuracy: {acc_log:.4f} ({acc_log*100:.2f}%)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"‚úì Tested {len(results_log)} configurations with log-transformed features\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mcwvf6zhg4",
   "source": "## 26. Compare Original vs Log-Transformed Performance\n\nCompare the best results from both approaches.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sb8rqcmzxda",
   "source": "# Compare original vs log-transformed\nprint(\"=\"*80)\nprint(\"ORIGINAL vs LOG-TRANSFORMED COMPARISON\")\nprint(\"=\"*80)\n\n# Get best from original\nsorted_results_orig = sorted(results, key=lambda x: x['accuracy'], reverse=True)\nbest_orig = sorted_results_orig[0]\n\n# Get best from log-transformed\nsorted_results_log = sorted(results_log, key=lambda x: x['accuracy'], reverse=True)\nbest_log = sorted_results_log[0]\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"BEST CONFIGURATION - ORIGINAL FEATURES\")\nprint(\"=\"*80)\nprint(f\"  Smoothing: {best_orig['smoothing']}\")\nprint(f\"  Min Variance: {best_orig['min_variance']:.1e}\")\nprint(f\"  Accuracy: {best_orig['accuracy']:.4f} ({best_orig['accuracy']*100:.2f}%)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"BEST CONFIGURATION - LOG-TRANSFORMED FEATURES\")\nprint(\"=\"*80)\nprint(f\"  Smoothing: {best_log['smoothing']}\")\nprint(f\"  Min Variance: {best_log['min_variance']:.1e}\")\nprint(f\"  Accuracy: {best_log['accuracy']:.4f} ({best_log['accuracy']*100:.2f}%)\")\n\n# Calculate improvement\nimprovement = (best_log['accuracy'] - best_orig['accuracy']) * 100\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PERFORMANCE COMPARISON\")\nprint(\"=\"*80)\n\nif best_log['accuracy'] > best_orig['accuracy']:\n    print(f\"‚úì Log transformation IMPROVED accuracy by {improvement:.2f}%\")\n    print(f\"  Original:        {best_orig['accuracy']:.4f}\")\n    print(f\"  Log-Transformed: {best_log['accuracy']:.4f}\")\n    print(f\"  Gain:            +{improvement:.2f}%\")\nelif best_log['accuracy'] < best_orig['accuracy']:\n    print(f\"‚ö† Log transformation DECREASED accuracy by {abs(improvement):.2f}%\")\n    print(f\"  Original:        {best_orig['accuracy']:.4f}\")\n    print(f\"  Log-Transformed: {best_log['accuracy']:.4f}\")\n    print(f\"  Loss:            {improvement:.2f}%\")\nelse:\n    print(f\"‚Üí Log transformation had NO IMPACT on accuracy\")\n    print(f\"  Both:            {best_orig['accuracy']:.4f}\")\n\n# Show top 5 from each\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOP 5 CONFIGURATIONS COMPARISON\")\nprint(\"=\"*80)\n\nprint(\"\\nOriginal Features:\")\nprint(f\"{'Rank':<6} {'Smoothing':<12} {'MinVar':<14} {'Accuracy'}\")\nprint(\"-\"*50)\nfor rank, result in enumerate(sorted_results_orig[:5], 1):\n    print(f\"{rank:<6} {result['smoothing']:<12.1f} {result['min_variance']:<14.1e} {result['accuracy']:.4f}\")\n\nprint(\"\\nLog-Transformed Features:\")\nprint(f\"{'Rank':<6} {'Smoothing':<12} {'MinVar':<14} {'Accuracy'}\")\nprint(\"-\"*50)\nfor rank, result in enumerate(sorted_results_log[:5], 1):\n    print(f\"{rank:<6} {result['smoothing']:<12.1f} {result['min_variance']:<14.1e} {result['accuracy']:.4f}\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "j23b4eb04ng",
   "source": "## 27. Visualize Original vs Log-Transformed Comparison\n\nCreate ROC curve comparison and accuracy distribution plots.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wc3ks7rg8t",
   "source": "# Visualize comparison\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n\n# LEFT: ROC Curves Comparison (Best Models)\nax1.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier', alpha=0.3)\n\n# Original best model ROC\nfpr_orig, tpr_orig, auc_orig = compute_roc_curve(y_test, best_orig['y_scores'])\nax1.plot(fpr_orig, tpr_orig, linewidth=3, color='#3498db', \n         label=f'Original (AUC={auc_orig:.3f}, Acc={best_orig[\"accuracy\"]:.3f})', alpha=0.8)\n\n# Log-transformed best model ROC\nfpr_log, tpr_log, auc_log = compute_roc_curve(y_test, best_log['y_scores'])\nax1.plot(fpr_log, tpr_log, linewidth=3, color='#e74c3c', \n         label=f'Log-Transformed (AUC={auc_log:.3f}, Acc={best_log[\"accuracy\"]:.3f})', alpha=0.8)\n\nax1.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nax1.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nax1.set_title('ROC Curve Comparison: Best Models', fontsize=13, fontweight='bold', pad=15)\nax1.legend(loc='lower right', fontsize=11)\nax1.grid(True, alpha=0.3, linestyle='--')\nax1.set_xlim([-0.02, 1.02])\nax1.set_ylim([-0.02, 1.02])\n\n# RIGHT: Accuracy Distribution Across All Configurations\naccuracies_orig = [r['accuracy'] for r in results]\naccuracies_log = [r['accuracy'] for r in results_log]\n\nx_pos = np.arange(len(accuracies_orig))\nwidth = 0.35\n\nbars1 = ax2.bar(x_pos - width/2, sorted(accuracies_orig, reverse=True), width, \n                label='Original', color='#3498db', alpha=0.7, edgecolor='black')\nbars2 = ax2.bar(x_pos + width/2, sorted(accuracies_log, reverse=True), width, \n                label='Log-Transformed', color='#e74c3c', alpha=0.7, edgecolor='black')\n\nax2.axhline(y=best_orig['accuracy'], color='#3498db', linestyle='--', linewidth=2, alpha=0.5)\nax2.axhline(y=best_log['accuracy'], color='#e74c3c', linestyle='--', linewidth=2, alpha=0.5)\n\nax2.set_xlabel('Configuration Rank', fontsize=12, fontweight='bold')\nax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\nax2.set_title('Accuracy Distribution Across All Configurations', fontsize=13, fontweight='bold', pad=15)\nax2.legend(loc='upper right', fontsize=11)\nax2.grid(True, alpha=0.3, axis='y', linestyle='--')\n\nplt.tight_layout()\n\n# Save the plot\noutput_path = Path.home() / \"Documents\" / \"OnlinePurchaseRetentionSTAT441\" / \"naive_bayes_plots\" / \"log_transform_comparison.png\"\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\nprint(f\"‚úì Plot saved to: {output_path}\")\n\nplt.show()\n\n# Summary statistics\nprint(\"\\n\" + \"=\"*80)\nprint(\"ACCURACY STATISTICS\")\nprint(\"=\"*80)\n\nprint(\"\\nOriginal Features:\")\nprint(f\"  Mean accuracy:   {np.mean(accuracies_orig):.4f}\")\nprint(f\"  Std deviation:   {np.std(accuracies_orig):.4f}\")\nprint(f\"  Min accuracy:    {np.min(accuracies_orig):.4f}\")\nprint(f\"  Max accuracy:    {np.max(accuracies_orig):.4f}\")\n\nprint(\"\\nLog-Transformed Features:\")\nprint(f\"  Mean accuracy:   {np.mean(accuracies_log):.4f}\")\nprint(f\"  Std deviation:   {np.std(accuracies_log):.4f}\")\nprint(f\"  Min accuracy:    {np.min(accuracies_log):.4f}\")\nprint(f\"  Max accuracy:    {np.max(accuracies_log):.4f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"AUC STATISTICS\")\nprint(\"=\"*80)\nprint(f\"  Original AUC:        {auc_orig:.4f}\")\nprint(f\"  Log-Transformed AUC: {auc_log:.4f}\")\nprint(f\"  Difference:          {(auc_log - auc_orig):.4f}\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "31745trfrg1",
   "source": "# Parameter Visualization & Feature Importance\n\nVisualize the learned parameters (probability distributions) and rank features by their importance in the Naive Bayes classifier.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "zl38cukcbbq",
   "source": "## 20. Feature Importance Calculation\n\nCalculate feature importance based on the discriminative power of each feature between classes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ggreytrx1i",
   "source": "import numpy as np\n\ndef calculate_feature_importance(model, columns, numeric_idx, categorical_idx):\n    \"\"\"\n    Calculate feature importance for Naive Bayes classifier.\n    \n    For numerical features: Use normalized mean difference (Cohen's d)\n    For categorical features: Use KL divergence or entropy difference\n    \"\"\"\n    importance_scores = {}\n    \n    # Calculate importance for numerical features\n    for col_idx in numeric_idx:\n        col_name = columns[col_idx]\n        \n        # Get mean and variance for both classes\n        mean_0, var_0 = model.num_stats[0][col_idx]\n        mean_1, var_1 = model.num_stats[1][col_idx]\n        \n        # Cohen's d: normalized difference in means\n        pooled_std = np.sqrt((var_0 + var_1) / 2)\n        cohens_d = abs(mean_1 - mean_0) / pooled_std if pooled_std > 0 else 0\n        \n        importance_scores[col_name] = {\n            'importance': cohens_d,\n            'type': 'numerical',\n            'mean_class_0': mean_0,\n            'mean_class_1': mean_1,\n            'std_class_0': np.sqrt(var_0),\n            'std_class_1': np.sqrt(var_1)\n        }\n    \n    # Calculate importance for categorical features\n    for col_idx in categorical_idx:\n        col_name = columns[col_idx]\n        \n        # Get category counts for both classes\n        counts_0 = model.cat_counts[0][col_idx]\n        counts_1 = model.cat_counts[1][col_idx]\n        total_0 = model.cat_totals[0][col_idx]\n        total_1 = model.cat_totals[1][col_idx]\n        \n        # Get all unique categories\n        all_categories = set(counts_0.keys()) | set(counts_1.keys())\n        \n        # Calculate KL divergence (simplified)\n        kl_divergence = 0.0\n        for category in all_categories:\n            # Add smoothing\n            p0 = (counts_0.get(category, 0) + 1) / (total_0 + len(all_categories))\n            p1 = (counts_1.get(category, 0) + 1) / (total_1 + len(all_categories))\n            \n            # KL divergence: sum of p1 * log(p1/p0)\n            if p1 > 0 and p0 > 0:\n                kl_divergence += p1 * np.log(p1 / p0)\n        \n        importance_scores[col_name] = {\n            'importance': abs(kl_divergence),\n            'type': 'categorical',\n            'n_categories': len(all_categories),\n            'counts_0': dict(counts_0),\n            'counts_1': dict(counts_1)\n        }\n    \n    return importance_scores\n\n# Calculate importance using the optimized model\nprint(\"=\"*60)\nprint(\"CALCULATING FEATURE IMPORTANCE\")\nprint(\"=\"*60)\n\nimportance_scores = calculate_feature_importance(\n    optimized_model, \n    columns, \n    numeric_idx, \n    categorical_idx\n)\n\n# Sort by importance\nsorted_features = sorted(\n    importance_scores.items(), \n    key=lambda x: x[1]['importance'], \n    reverse=True\n)\n\nprint(f\"\\n{'Rank':<6} {'Feature':<30} {'Type':<12} {'Importance'}\")\nprint(\"-\"*70)\n\nfor rank, (feature_name, info) in enumerate(sorted_features, 1):\n    marker = \"‚≠ê\" if rank <= 5 else \"\"\n    print(f\"{rank:<6} {feature_name:<30} {info['type']:<12} {info['importance']:.4f} {marker}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úì Feature importance calculated\")\nprint(\"  - Numerical features: Cohen's d (effect size)\")\nprint(\"  - Categorical features: KL divergence\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b19temvqed9",
   "source": "## 21. Numerical Feature Parameters (Best Model)\n\nVisualize the learned Gaussian distributions (mean ¬± std) for each numerical feature. These parameters are from the best performing model (Smoothing=10.0, MinVar=0.1).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "t93t97ipmjh",
   "source": "# Visualize numerical feature parameters from BEST model\nnumerical_features = [(name, info) for name, info in sorted_features if info['type'] == 'numerical']\n\nfig, axes = plt.subplots(5, 2, figsize=(16, 20))\naxes = axes.flatten()\n\nprint(\"=\"*60)\nprint(\"NUMERICAL FEATURE PARAMETERS (BEST MODEL)\")\nprint(f\"Best Model: Smoothing={best['smoothing']}, MinVar={best['min_variance']:.1e}\")\nprint(\"=\"*60)\nprint(f\"\\n{'Feature':<30} {'Class 0 (No Purchase)':<25} {'Class 1 (Purchase)'}\")\nprint(\"-\"*80)\n\nfor idx, (feature_name, info) in enumerate(numerical_features):\n    ax = axes[idx]\n    \n    mean_0 = info['mean_class_0']\n    mean_1 = info['mean_class_1']\n    std_0 = info['std_class_0']\n    std_1 = info['std_class_1']\n    \n    # Print statistics\n    print(f\"{feature_name:<30} Œº={mean_0:>8.3f} œÉ={std_0:>8.3f}   Œº={mean_1:>8.3f} œÉ={std_1:>8.3f}\")\n    \n    # Create x range for plotting distributions\n    x_min = min(mean_0 - 3*std_0, mean_1 - 3*std_1)\n    x_max = max(mean_0 + 3*std_0, mean_1 + 3*std_1)\n    x = np.linspace(x_min, x_max, 200)\n    \n    # Calculate Gaussian PDFs\n    y_0 = (1 / (std_0 * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean_0) / std_0) ** 2)\n    y_1 = (1 / (std_1 * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean_1) / std_1) ** 2)\n    \n    # Plot distributions\n    ax.fill_between(x, y_0, alpha=0.4, color='#e74c3c', label=f'No Purchase (Œº={mean_0:.2f})')\n    ax.fill_between(x, y_1, alpha=0.4, color='#2ecc71', label=f'Purchase (Œº={mean_1:.2f})')\n    ax.plot(x, y_0, color='#c0392b', linewidth=2)\n    ax.plot(x, y_1, color='#27ae60', linewidth=2)\n    \n    # Add vertical lines for means\n    ax.axvline(mean_0, color='#c0392b', linestyle='--', linewidth=1.5, alpha=0.7)\n    ax.axvline(mean_1, color='#27ae60', linestyle='--', linewidth=1.5, alpha=0.7)\n    \n    # Labels and title\n    ax.set_xlabel('Value', fontsize=10)\n    ax.set_ylabel('Probability Density', fontsize=10)\n    ax.set_title(f\"{feature_name} (Importance: {info['importance']:.3f})\", \n                 fontsize=11, fontweight='bold', pad=10)\n    ax.legend(loc='best', fontsize=8)\n    ax.grid(True, alpha=0.3, linestyle='--')\n\n# Remove empty subplots if any\nfor idx in range(len(numerical_features), len(axes)):\n    fig.delaxes(axes[idx])\n\nplt.tight_layout(rect=[0, 0, 1, 0.98])\n\n# Save the plot\noutput_path = Path.home() / \"Documents\" / \"OnlinePurchaseRetentionSTAT441\" / \"naive_bayes_plots\" / \"numerical_parameters.png\"\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\nprint(f\"\\n‚úì Plot saved to: {output_path}\")\n\nplt.show()\n\nprint(\"\\n‚úì Numerical feature parameters visualized\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8bsueoog0qf",
   "source": "## 22. Categorical Feature Parameters (Best Model)\n\nVisualize the learned probability distributions for categorical features. These parameters are from the best performing model (Smoothing=10.0, MinVar=0.1).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p1aii8fbffc",
   "source": "# Visualize categorical feature parameters from BEST model\ncategorical_features = [(name, info) for name, info in sorted_features if info['type'] == 'categorical']\n\n# Create subplots for categorical features\nn_cat_features = len(categorical_features)\nfig, axes = plt.subplots(4, 2, figsize=(18, 18))\naxes = axes.flatten()\n\nprint(\"=\"*60)\nprint(\"CATEGORICAL FEATURE PARAMETERS (BEST MODEL)\")\nprint(f\"Best Model: Smoothing={best['smoothing']}, MinVar={best['min_variance']:.1e}\")\nprint(\"=\"*60)\n\nfor idx, (feature_name, info) in enumerate(categorical_features):\n    ax = axes[idx]\n    \n    counts_0 = info['counts_0']\n    counts_1 = info['counts_1']\n    \n    # Get total counts for probability calculation\n    total_0 = sum(counts_0.values())\n    total_1 = sum(counts_1.values())\n    \n    # Get all categories\n    all_categories = sorted(set(counts_0.keys()) | set(counts_1.keys()))\n    \n    # Limit to top 10 categories by total count if there are too many\n    if len(all_categories) > 10:\n        category_totals = {cat: counts_0.get(cat, 0) + counts_1.get(cat, 0) for cat in all_categories}\n        all_categories = sorted(category_totals.keys(), key=lambda x: category_totals[x], reverse=True)[:10]\n        title_suffix = \" (Top 10)\"\n    else:\n        title_suffix = \"\"\n    \n    # Calculate probabilities\n    probs_0 = [counts_0.get(cat, 0) / total_0 for cat in all_categories]\n    probs_1 = [counts_1.get(cat, 0) / total_1 for cat in all_categories]\n    \n    # Print statistics\n    print(f\"\\n{feature_name}:\")\n    print(f\"  - Number of categories: {info['n_categories']}\")\n    print(f\"  - KL Divergence: {info['importance']:.4f}\")\n    \n    # Create bar positions\n    x = np.arange(len(all_categories))\n    width = 0.35\n    \n    # Plot bars\n    bars1 = ax.bar(x - width/2, probs_0, width, label='No Purchase', \n                   color='#e74c3c', alpha=0.7, edgecolor='black', linewidth=1)\n    bars2 = ax.bar(x + width/2, probs_1, width, label='Purchase', \n                   color='#2ecc71', alpha=0.7, edgecolor='black', linewidth=1)\n    \n    # Labels and title\n    ax.set_xlabel('Category', fontsize=10, fontweight='bold')\n    ax.set_ylabel('Probability', fontsize=10, fontweight='bold')\n    ax.set_title(f\"{feature_name}{title_suffix} (Importance: {info['importance']:.3f})\", \n                 fontsize=11, fontweight='bold', pad=12)\n    ax.set_xticks(x)\n    ax.set_xticklabels(all_categories, rotation=45, ha='right', fontsize=8)\n    ax.legend(loc='best', fontsize=9)\n    ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n    ax.set_ylim([0, max(max(probs_0), max(probs_1)) * 1.15])\n    \n    # Add value labels on bars for significant probabilities\n    for bar in bars1:\n        height = bar.get_height()\n        if height > 0.05:  # Only label if probability > 5%\n            ax.text(bar.get_x() + bar.get_width()/2., height,\n                   f'{height:.2f}', ha='center', va='bottom', fontsize=7)\n    \n    for bar in bars2:\n        height = bar.get_height()\n        if height > 0.05:\n            ax.text(bar.get_x() + bar.get_width()/2., height,\n                   f'{height:.2f}', ha='center', va='bottom', fontsize=7)\n\n# Remove empty subplots if any\nfor idx in range(len(categorical_features), len(axes)):\n    fig.delaxes(axes[idx])\n\nplt.tight_layout(rect=[0, 0, 1, 0.98])\n\n# Save the plot\noutput_path = Path.home() / \"Documents\" / \"OnlinePurchaseRetentionSTAT441\" / \"naive_bayes_plots\" / \"categorical_parameters.png\"\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\nprint(f\"\\n‚úì Plot saved to: {output_path}\")\n\nplt.show()\n\nprint(\"\\n‚úì Categorical feature parameters visualized\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "45s7mbgyf2d",
   "source": "## 23. Overall Feature Importance Ranking\n\nVisualize the ranked importance of all features in the Naive Bayes model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "voysvtxdngi",
   "source": "# Create feature importance ranking visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n\n# LEFT PLOT: All features ranked by importance\nfeature_names = [name for name, _ in sorted_features]\nimportance_values = [info['importance'] for _, info in sorted_features]\nfeature_types = [info['type'] for _, info in sorted_features]\n\n# Color by type\ncolors = ['#3498db' if ftype == 'numerical' else '#e67e22' for ftype in feature_types]\n\ny_pos = np.arange(len(feature_names))\n\nbars = ax1.barh(y_pos, importance_values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n\n# Add value labels\nfor i, (bar, val) in enumerate(zip(bars, importance_values)):\n    ax1.text(val + max(importance_values) * 0.01, bar.get_y() + bar.get_height()/2, \n             f'{val:.3f}', va='center', fontsize=9, fontweight='bold')\n\nax1.set_yticks(y_pos)\nax1.set_yticklabels(feature_names, fontsize=10)\nax1.invert_yaxis()  # Highest importance at top\nax1.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\nax1.set_title('All Features Ranked by Importance', fontsize=13, fontweight='bold', pad=15)\nax1.grid(True, alpha=0.3, axis='x', linestyle='--')\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='#3498db', edgecolor='black', label='Numerical (Cohen\\'s d)'),\n    Patch(facecolor='#e67e22', edgecolor='black', label='Categorical (KL Divergence)')\n]\nax1.legend(handles=legend_elements, loc='lower right', fontsize=10)\n\n# RIGHT PLOT: Top 10 features with detailed breakdown\ntop_10_features = sorted_features[:10]\ntop_10_names = [name for name, _ in top_10_features]\ntop_10_values = [info['importance'] for _, info in top_10_features]\ntop_10_types = [info['type'] for _, info in top_10_features]\n\n# Color by type with gradient based on rank\ncolors_top10 = []\nfor i, ftype in enumerate(top_10_types):\n    if ftype == 'numerical':\n        # Blue gradient\n        intensity = 1 - (i / 20)  # Darker for more important\n        colors_top10.append((0.2, 0.6 * intensity + 0.4, 0.86))\n    else:\n        # Orange gradient\n        intensity = 1 - (i / 20)\n        colors_top10.append((0.9, 0.49 * intensity + 0.51, 0.13))\n\ny_pos_top10 = np.arange(len(top_10_names))\n\nbars2 = ax2.barh(y_pos_top10, top_10_values, color=colors_top10, \n                 alpha=0.8, edgecolor='black', linewidth=2)\n\n# Add rank labels\nfor i, (bar, val, name) in enumerate(zip(bars2, top_10_values, top_10_names)):\n    # Rank badge\n    ax2.text(-max(top_10_values) * 0.05, bar.get_y() + bar.get_height()/2, \n             f'#{i+1}', va='center', ha='right', fontsize=11, \n             fontweight='bold', color='white',\n             bbox=dict(boxstyle='circle', facecolor='#34495e', edgecolor='white', linewidth=2))\n    \n    # Value label\n    ax2.text(val + max(top_10_values) * 0.01, bar.get_y() + bar.get_height()/2, \n             f'{val:.3f}', va='center', fontsize=10, fontweight='bold')\n\nax2.set_yticks(y_pos_top10)\nax2.set_yticklabels(top_10_names, fontsize=11, fontweight='bold')\nax2.invert_yaxis()\nax2.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\nax2.set_title('Top 10 Most Discriminative Features', \n              fontsize=13, fontweight='bold', pad=15)\nax2.grid(True, alpha=0.3, axis='x', linestyle='--')\nax2.set_xlim([-max(top_10_values) * 0.1, max(top_10_values) * 1.25])\n\nplt.tight_layout()\n\n# Save the plot\noutput_path = Path.home() / \"Documents\" / \"OnlinePurchaseRetentionSTAT441\" / \"naive_bayes_plots\" / \"feature_importance_ranking.png\"\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\nprint(f\"‚úì Plot saved to: {output_path}\")\n\nplt.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FEATURE IMPORTANCE SUMMARY\")\nprint(\"=\"*80)\nprint(\"\\nTop 5 Most Important Features:\")\nfor i, (name, info) in enumerate(sorted_features[:5], 1):\n    print(f\"  {i}. {name:<30} ({info['type']:<12}) - {info['importance']:.4f}\")\n\nprint(\"\\nBottom 5 Least Important Features:\")\nfor i, (name, info) in enumerate(sorted_features[-5:], len(sorted_features)-4):\n    print(f\"  {i}. {name:<30} ({info['type']:<12}) - {info['importance']:.4f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"KEY INSIGHTS\")\nprint(\"=\"*80)\nprint(\"‚Ä¢ Cohen's d (numerical): Measures effect size (difference in means)\")\nprint(\"  - Small: 0.2 | Medium: 0.5 | Large: 0.8+\")\nprint(\"‚Ä¢ KL Divergence (categorical): Measures distribution difference\")\nprint(\"  - Higher values = more discriminative between classes\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4d6e2c0c",
   "metadata": {},
   "source": [
    "## Results & Notes- Existing models (logistic regression, NN, tuned RF, tuned XGBoost) achieve ~0.87-0.90 accuracy.- Naive Bayes supplies a lightweight baseline (~0.81 accuracy) despite mixed numerical/categorical features.- Use `python3 naive_bayes_baseline.py` or run the cell above to reproduce the printed comparison table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}